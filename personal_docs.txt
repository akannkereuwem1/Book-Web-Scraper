
scrapy command:
bench         Run quick benchmark test
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy
    
Command to run scrapy: scrapy crawl book

#NOTE

    books/ is the root directory of your Scrapy project.
    books/spiders/ is a directory to store your spider definitions.
    books/items.py is a file to define the data structure of your scraped items.
    books/middlewares.py is a file to manage your custom middlewares.
    books/pipelines.py is a file to define item processing pipelines.
    books/settings.py is the settings file to configure your project.


#TIP
Before writing a full-fledged spider, it’s often useful to preview and test how you’ll extract data from a webpage. The Scrapy shell is an interactive tool that lets you inspect and extract data from web pages in real time. This helps in experimenting with XPath expressions and CSS selectors to ensure they correctly target the elements you’re looking for.

#TIP
Selector 	Effect
1.  h3 and a 	Targets elements of that HTML element type
2.  > 	Indicates a child element
3.  .price_color and article.product_pod 	Indicates a class name and, optionally, specifies on which element the class name should be
4.  ::text 	Targets the text content of a HTML tag
5.  ::atr(href) 	Targets the value of an HTML attribute, in this case the URL in an href attribute


Note: Using yield turns .parse() into a generator that yields its results rather than returning them. This allows the framework to handle multiple requests and responses concurrently.

Scrapy is built on top of the Twisted framework, which is an event-driven networking engine that allows for asynchronous requests. This makes the scraping process more efficient and scalable.

#Note
settings.py is a central place that allows you to define settings for your project, and it already contains some information. Later on, this variables are to be loaded to the pipeline

#Noten item pipeline is a Python class that defines several methods to process items after your spider has scraped them from the Internet:

    .open_spider() gets called when the spider opens.
    .close_spider() gets called when the spider closes.
    .process_item() gets called for every item pipeline component. It must either return an item or raise a DropItem exception.
    .from_crawler() creates a pipeline from a Crawler in order to make the general project settings available to the pipeline.


#Note
While loading data into a database is one possible use case of an item pipeline, you can also use them for other purposes, such as cleaning HTML data or validating scraped data.

#The ETL process is a common way to handle data and stands for Extract, Transform, and Load. 

#Note
item pipeline is a series of steps within the scraping program that an "item" (the data you extracted) passes through. It's called a pipeline because it's a series of stages, one after another

##NOTE


    itemadapter wraps different data containers to handle them in a uniform manner. The package was installed as a dependency of Scrapy.

    COLLECTION_NAME specifies the name of the MongoDB collection where you want to store the items. This should match the name of the collection that you set up earlier.

    .__init__() initializes the pipeline with the MongoDB URI and database name. You can access this information because you’re fetching it from the Crawler using the .from_crawler() class method.

    .from_crawler() is a class method that gives you access to all core Scrapy components, such as the settings. In this case, you use it to retrieve the MongoDB settings from settings.py through the Scrapy crawler.

    .open_spider() opens a connection to MongoDB when the spider starts.

    .close_spider() closes the MongoDB connection when the spider finishes.

    .process_item() inserts each scraped item into the MongoDB collection. This method usually contains the core functionality of a pipeline.


#IMPORTANT
You can add pipelines to your project as entries in a dictionary, where the qualified name of your pipeline class is the key, and an integer is the value. The integer determines the order in which Scrapy runs the pipelines. Lower numbers mean higher priority.


###IMPORTANT TO NOTE

    Your spider adds values for .url, .title, and .price to a BooksItem.
    Your item pipeline adds the value for ._id, after calculating it from .url.
  
  .compute_item_id() uses Python’s hashlib to hash the URL value. 
  Using the hashed URL as a unique identifier means that Python won’t add any book that you scraped twice from the same URL to your collection.

####MongoDB jot down

To activate the mongo shell: sudo docker exec -it mongodb mongo

CHAT_GPT_CHAT_NAME = edit commit message

###NOTE
in older versions of the mongo shell (like 4.0 and below), countDocuments() requires you to 
explicitly pass a filter object — even if you don’t want to filter anything.


INSTEAD OF : db.books.countDocuments() use db.books.countDocuments({})
or count onlyy one document by using the example syntax : 
  db.books.countDocuments({ author: "Daniel" })